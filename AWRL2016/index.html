<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AWRL 2016</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"><style type="text/css">
<!--
body {
	background-color: #FFFFFF;
}
.STYLE1 {color: #FF0000}
.STYLE2 {color: #FF0000; font-weight: bold; }
.STYLE3 {color: #006600; font-weight: bold; }
.STYLE5 {font-size: x-large}
-->
</style></head>
<body style="background-color: lightgray ">
    <div id="wrapper">
        <div id="header" style="display: table; margin: 0 auto">
            <p><img src="img/title.png" width="957px" height="141px"></p>
            <h3 align="center" class="STYLE2"><span class="STYLE5">The First Asian Workshop on Reinforcement Learning (AWRL 2016)</span><br>
            The University of Waikato, Hamilton - November 16, 2016</h3>
            <p align="center" class="STYLE3">In conjunction with the 8th Asian Conference on Machine Learning (<a href="http://www.acml-conf.org/2016/">ACML 2016</a>) </p>
      </div>

	    <div id="main">
            <div class="content" style="width:900px; display: table; margin: 0 auto">
					<span class="title"><h2><b>Latest News</b></h2></span>
					 <p class="pre">
						<a href="https://scholar.google.com/citations?user=8K8-LdwAAAAJ&hl=en">Olivier Pietquin</a> from Deepmind and University Lille 1 (France) will give an invited talk at AWRL 2016! 
<p>
<em>Title:</em> Closing the interaction loop with (inverse) reinforcement learning
</p>
<p>
<em>Abstract:</em> Modern interactive systems are mostly based on an aggregation of Machine Learning (ML) modules that are trained on batch data (e.g. automatic speech, gesture or emotion recognition, language understanding or generation, text-to-speech synthesis, etc.). Yet, closing the interaction loop with ML-based techniques is still an issue because keeping the human in the learning loop raises new challenges for ML (such as non-stationarity, subjective evaluation, risk aversion, safe exploration, etc.). In this talk, I will discuss a Reinforcement Learning (RL) approach to this problem and show how some of these challenges can be addressed with direct and especially inverse RL methods in the context of Markov Decision Processes.
</p>
					 </p>
                <p class="pre">
                  <a href="https://www.microsoft.com/en-us/research/people/lihongli/">Lihong Li</a> from Microsoft Research will give an invited talk at AWRL 2016!
                </p>

<span class="title"><h2><b>Program</b></h2></span>
<ul>
<li><em>07:40</em> Registration Open - S Block Foyer</li>
<li>08:50-09:00 Introduction</li>
<li><em>09:00-10:00</em> Keynote 1 by <b>Olivier Pietquin</b> (Deepmind and University Lille 1, France)</li>
<li><em>10:00-10:20</em> Break</li>
<li><em>10:20-12:00</em> Session 1 (4 talks, 20m + 5m questions each)</li>
<ul>
<li><a href="https://arxiv.org/abs/1611.02053">Reinforcement-based Simultaneous Algorithm and its Hyperparameters Selection</a> by Valeria Efimova, <b>Andrey Filchenkov</b>, Anatoly Shalyto</li>
<li><a href="https://arxiv.org/abs/1611.00862">Quantile Reinforcement Learning</a> by <b>Hugo Gilbert</b> and Paul Weng</li>
<li><a href="">Reinforcement Learning with Skew-symmetric Bilinear Utility Functionals</a> by Hugo Gilbert and <b>Paul Weng</b></li>
<li><a href="https://arxiv.org/abs/1611.02047">Reinforcement Learning Approach for Parallelization in Filters Aggregation Based Feature Selection Algorithms</a> by Ivan Smetannikov, Ilya Isaev and <b>Andrey Filchenkov</b></li>
</ul>
<li><em>13:00-14:00</em> ACML Industry Keynote (Room: S.1.04)</li>

<li><em>14:15-15:15</em> Keynote 2 by <b>Lihong Li</b> (Microsoft)</li>

<li><em>15:15-15:40</em> Session 2 (1 talk, 20m + 5m questions)</li>
<ul>
<li><a href="">Learning Functional Meta-Policy over Parameterized Task Distribution</a> by Qing Da, <b>Yang Yu</b> and Zhi-Hua Zhou</li>
</ul>
<li><em>15:40-16:00</em> Break</li>
<li><em>16:00-16:50</em> Session 3 (2 talks, 20m + 5m questions each)</li>
<ul>
<li><a href="https://arxiv.org/abs/1611.00873">Extracting Actionability from Machine Learning Models by Sub-optimal Deterministic Planning</a> by <b>Qiang Lyu</b>, Yixin Chen, Zhaorong Li, Zhicheng Cui, Ling Chen, Xing Zhang and Haihua Shen</li>
<li><a href="">Classification-based Optimization for Direct Policy Search</a> by <b>Yang Yu</b></li>
</ul>
<li><em>16:50-17:40</em> Free discussions</li>
</ul>

            	<span class="title"><h2><b>Motivation</b></h2></span>
                <p class="pre">
                    The first Asian Workshop on Reinforcement Learning (AWRL 2016) focuses on both theoretical models and algorithms of reinforcement learning (RL) and its practical applications. In the last few years, we have seen the growing interest in RL of researchers from different research areas and industries. We invite reinforcement learning researchers and practitioners to participate in this world-class gathering. We intend to make this an exciting event for researchers and practitioners in RL worldwide, not only for the presentation of top quality papers, but also as a forum for the discussion of open problems, future research directions and application domains of RL. AWRL 2016 will consist of keynote talks (TBA), contributed paper presentations, discussion sessions spread over a one-day period.
                </p>

                <p>
                    Reinforcement learning (RL) is an active field of research that deals with the problem of (single or multiple agents') sequential decision-making in unknown possibly partially observable domains, whose (potentially non-stationary) dynamics may be deterministic, stochastic or adversarial. RL's objective is to develop agents' capability of learning optimal policies in unknown environments (possibly in face of other coexisting agents) by trial-and-error and with limited supervision. Recent developments in exploration-exploitation, online learning, planning, and representation learning are making RL more and more appealing to real-world applications, with promising results in challenging domains such as recommendation systems, computer games, or robotics systems. We would like to create a forum to discuss interesting results both theoretically and empirically related with RL. The ultimate goal of this workshop is to bring together diverse viewpoints in the RL area in an attempt to consolidate the common ground, identify new research directions, and promote the rapid advance of RL research community.
                </p>

                <span class="title"><h2><b>Topics</b></h2></span>
                <div class="line">
                    The workshop will cover a range of sub-topics in RL, from theoretical aspects to empirical evaluations, including but not limited to:
                </div>
                <ul>
                    <li>Exploration/Exploitation</li>
                    <li>Function approximation in RL</li>
                    <li>Deep RL</li>
                    <li>Policy search methods</li>
                    <li>Batch RL </li>
                    <li>Kernel methods for RL</li>
                    <li>Evolutionary RL </li>
                    <li>Partially observable RL</li>
                    <li>Bayesian RL</li>
                    <li>Multi-agent RL</li>
						  <li>RL in non-stationary domains</li>
                    <li>Life-long RL</li>
						  <li>Non-standard Criteria in RL, e.g.:
						  <ul>
                    		<li>Risk-sensitive RL</li>
								<li>Multi-objective RL</li>
								<li>Preference-based RL</li>
						  </ul>
						  </li>
                    <li>Transfer Learning in RL</li>
                    <li>Knowledge Representation in RL</li>
						  <li>Hierarchical RL</li>
                    <li>Interactive RL</li>
						  <li>RL in psychology and neuroscience</li>
						  <li>Applications of RL, e.g.:
						  <ul>
								<li>Recommender systems</li>
								<li>Robotics</li>
								<li>Video games</li>
								<li>Finance</li>
							</ul>
							</li>
                </ul>

                <span class="title"><h2><b>Paper Submission</b></h2></span>
					 <p class="pre">
						Workshop submissions and camera ready versions will be handled by EasyChair. Click <a href="https://easychair.org/conferences/?conf=awrl2016">here</a> for submission.
					 </p>

					 <p class="pre">
						Papers should be formatted according to the ACML formatting instructions for the <a href="http://www.acml-conf.org/2016/authors/call-for-papers/">Conference Track</a>. 
						Submissions need not be anonymous.
					 </p>

					 <p class="pre">
						AWRL is a non-archival venue and there will be no published proceedings. 
						However, the papers will be posted on this website. 
						Therefore it will be possible to submit to other conferences and journals both in parallel to and after AWRL 2016. 
						Besides, we also welcome submissions to AWRL that are under review at other conferences and workshops. 
						For this reason, please feel free to submit either anonymized or non-anonymized versions of your work. 
						We have enabled anonymous reviewing so EasyChair will not reveal the authors unless you chose to do so in your PDF.
					 </p>

					 <p class="pre">
						At least one author from each accepted paper must register for the workshop. 
						Please see the <a href="http://www.acml-conf.org/2016/">ACML 2016 Website</a> for information about accommodation and registration.
					 </p>

                <span class="title"><h2><b>Important Dates</b></h2></span>
                <ul>
                    <li><em>Submission deadline:</em> Aug. 31, 2016 </li>
                    <li><em>Notification of acceptance:</em> Oct. 10, 2016</li>
                    <li><em>Camera ready deadline:</em> Oct. 24, 2016</li>
                    <li><em>Workshop date:</em> Nov. 16, 2016</li>
						  <li><em>ACML dates:</em> Nov. 16-18, 2016</li>
                </ul>

                <span class="title"><h2><b>Organizing Committee</b></h2></span>
                <p>
                    <em><a href="http://www.escience.cn/people/jianye/">Jianye Hao</a></em>, Tianjin University, China<br/>
                    <em><a href="http://weng.fr/">Paul Weng</a></em>, SYSU-CMU Joint Institute of Engineering, China<br/>
                    <em><a href="http://lamda.nju.edu.cn/yuy/">Yang Yu</a></em>, Nanjing University, China<br/>
                    <em><a href="http://www.escience.cn/people/zzzhang/">Zongzhang Zhang</a></em>, Soochow University, China<br/>
                </p>
            </div>
        </div>
    </div>
</body>
</html>



